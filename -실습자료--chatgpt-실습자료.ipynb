{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnPHkKA-ZCjJ"
   },
   "source": [
    "## chatrwkv 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2obyjxGZCIs",
    "outputId": "6173b28f-40a8-4503-997c-ea0beb23778a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rwkv\n",
      "  Downloading rwkv-0.7.5-py3-none-any.whl (395 kB)\n",
      "     -------------------------------------- 395.3/395.3 kB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\05455\\anaconda3\\lib\\site-packages (from rwkv) (0.13.3)\n",
      "Installing collected packages: rwkv\n",
      "Successfully installed rwkv-0.7.5\n",
      "Requirement already satisfied: tokenizers in c:\\users\\05455\\anaconda3\\lib\\site-packages (0.13.3)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/BlinkDL/ChatRWKV\n",
    "!pip install rwkv\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHgYWpT7aA3Y",
    "outputId": "8f343b87-5b75-40ef-eee0-be35be79b90c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델다운로드: https://huggingface.co/BlinkDL\n",
    "# https://huggingface.co/BlinkDL/rwkv-4-pile-169m/tree/main\n",
    "\n",
    "!wget https://huggingface.co/BlinkDL/rwkv-4-pile-169m/resolve/main/RWKV-4-Pile-169M-20220807-8023.pth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZQ34n27aoWH",
    "outputId": "534fa46a-13b1-44a7-a2e6-e0c94a094f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatRWKV'...\n",
      "remote: Enumerating objects: 1110, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 1110 (delta 0), reused 2 (delta 0), pack-reused 1103\u001b[K\n",
      "Receiving objects: 100% (1110/1110), 4.78 MiB | 15.80 MiB/s, done.\n",
      "Resolving deltas: 100% (634/634), done.\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 다운\n",
    "# !rm -rf ChatRWKV\n",
    "!git clone https://github.com/BlinkDL/ChatRWKV\n",
    "!mv ChatRWKV/20B_tokenizer.json ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpqETEkrZCLB"
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio\n",
    "# https://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_in_150_lines.py\n",
    "# https://pypi.org/project/rwkv/\n",
    "from rwkv.model import RWKV\n",
    "from rwkv.utils import PIPELINE, PIPELINE_ARGS\n",
    "\n",
    "def my_print(s):\n",
    "    print(s, end='', flush=True)\n",
    "\n",
    "model_path = 'RWKV-4-Pile-169M-20220807-8023'\n",
    "model = RWKV(model=model_path, strategy='cuda fp16 *32 -> cpu fp32') # strategy='cpu fp32'\n",
    "# https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "pipeline = PIPELINE(model, \"20B_tokenizer.json\") \n",
    "\n",
    "# For alpha_frequency and alpha_presence, see \"Frequency and presence penalties\":\n",
    "# https://platform.openai.com/docs/api-reference/parameter-details\n",
    "\n",
    "args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.7,\n",
    "                     alpha_frequency = 0.25,\n",
    "                     alpha_presence = 0.25,\n",
    "                     token_ban = [0], # ban the generation of some tokens\n",
    "                     token_stop = []) # stop generation whenever you see any token here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_4WTZA6ZCNd"
   },
   "outputs": [],
   "source": [
    "prompt = \"Do you know chatgpt?\" # \n",
    "print(prompt, end='')\n",
    "\n",
    "# 생성하기\n",
    "result = pipeline.generate(prompt, token_count=200, args=args, callback=my_print)\n",
    "print(result)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEAb37dKZCP4"
   },
   "outputs": [],
   "source": [
    "# 모델 logit 출력하기\n",
    "out, state = model.forward([187, 510, 1563, 310, 247], None)\n",
    "print(out.detach().cpu().numpy())                   # get logits\n",
    "out, state = model.forward([187, 510], None)\n",
    "out, state = model.forward([1563], state)           # RNN has state (use deepcopy to clone states)\n",
    "out, state = model.forward([310, 247], state)\n",
    "print(out.detach().cpu().numpy())                   # same result as above\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tgvsb21rZCST"
   },
   "outputs": [],
   "source": [
    "# 학습을 하고 싶다면?\n",
    "https://github.com/resloved/RWKV-notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMrSVjeVxra9"
   },
   "source": [
    "## chatGPT 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NeAh727yAKL",
    "outputId": "36b93571-abbd-430f-d611-84379831b589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Nd4KaMLxo7l"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openapi_key = 'your_key'\n",
    "openai.api_key = openapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6gA-YjZyHFf"
   },
   "outputs": [],
   "source": [
    "model=\"gpt-3.5-turbo\"\n",
    "query = \"How to learn English?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtJQN0wXxo-C"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ref: https://help.openai.com/en/articles/7042661-chatgpt-api-transition-guide\n",
    "    ref: https://wooiljeong.github.io/python/chatgpt-api/\n",
    "\n",
    "    messages: 생성하려는 대화의 히스토리를 정의하는데 사용됩니다. 대화에 참여하는 여러 역할(‘system(시스템)’, ‘assistant(도우미)’, ‘user(사용자)’)과 메시지 내용을 설정할 수 있습니다. \n",
    "    보통 대화는 먼저 시스템 메시지로 형식을 정의합니다. 그 다음 사용자와 도우미의 메시지를 번갈아가며 정의합니다.\n",
    "\n",
    "    시스템 메시지: ‘You are a helpful assistant.’와 같은 메시지로 도우미에게 지시할 수 있습니다. 시스템이 챗봇에게 일종의 역할을 부여한다고 볼 수 있습니다.\n",
    "    사용자 메시지: 도우미에게 직접 전달하는 내용입니다.\n",
    "    도우미 메시지: 이전에 응답했던 결과를 저장해 대화의 흐름을 유지할 수 있도록 설정할 수 있습니다.\n",
    "\"\"\"\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "messages1 = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that speaks like child.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "completion1 = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZSjulVgyrTm"
   },
   "outputs": [],
   "source": [
    "# print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGQWAJjAy4Kx",
    "outputId": "b2a625c8-8a4d-40b8-f2c8-1ddd05ba3069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I can provide you few tips on how to learn English:\n",
      "\n",
      "1. Start with the basics: Learn the English alphabet, grammar rules, and simple vocabulary. Setting a strong foundation is important to build upon.\n",
      "\n",
      "2. Watch and listen to English content: Watch TV shows, movies, news, and listen to English language songs, podcasts, or radio. This will help you train your ears to understand different accents, increase your vocabulary, and practice pronunciation.\n",
      "\n",
      "3. Have conversations in English: Find native speakers or language exchange partners to practice speaking and listening skills. Conversational practice is immensely helpful to improve fluency and clarity.\n",
      "\n",
      "4. Read English books and articles: Start with beginner-level books and gradually move to more advanced ones. This will help you develop reading comprehension, writing, and critical thinking skills.\n",
      "\n",
      "5. Take online English courses: There are numerous online courses available for different levels of proficiency. These courses provide interactive and structured learning modules with quizzes, assessments, and assignments.\n",
      "\n",
      "6. Practice consistently: Learning a language requires consistent practice. Set daily or weekly goals to improve your skills and stick to them. Revise and consolidate what you learned previously.\n",
      "\n",
      "7. Be patient and don’t give up: Learning a new language can be challenging, but don’t give up. Persevere even when it gets difficult, and keep practicing to achieve your language goals.\n"
     ]
    }
   ],
   "source": [
    "response = completion['choices'][0]['message']['content']\n",
    "print(response)\n",
    "\n",
    "# 1. Start by setting goals: Determine why you want to learn English and what your goals are. Do you want to improve your speaking skills, prepare for an exam, or improve your job prospects? Setting goals can help you focus your learning efforts.\n",
    "\n",
    "# 2. Practice regularly: Set aside some time every day or every week to practice English. This can include watching videos or movies, reading books or articles, practicing speaking with native speakers, writing paragraphs, or listening to English podcasts.\n",
    "\n",
    "# 3. Immerse yourself in English: Surround yourself with English as much as possible. Watch English-language TV shows and movies, listen to English music and radio, read English newspapers or books, and interact with English-speaking people as much as possible.\n",
    "\n",
    "# 4. Take classes: If you prefer a more structured approach to learning English, consider enrolling in classes at a language school or taking online courses. This can help you build a solid foundation in grammar, vocabulary, and conversation skills.\n",
    "\n",
    "# 5. Use technology to your advantage: There are many apps and online tools that can help you learn English. Duolingo, Babbel, and Busuu are popular language learning apps, and websites like BBC Learning English and ESLPod offer free resources.\n",
    "\n",
    "# 6. Practice speaking: Practice speaking with other English learners or native speakers. You can join language exchange groups or language exchange apps like iTalki, HelloTalk, or Tandem to find conversation partners.\n",
    "\n",
    "# 7. Don't be afraid to make mistakes: Learning a language involves making mistakes. Don't be afraid to practice speaking or writing in English, even if you make mistakes. Remember that mistakes are a natural part of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WlxoGp70EFZ",
    "outputId": "1e8cbe1b-a442-4e1b-e087-e71c43f1d59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning English can be so much fun! To start learning English, you can listen to songs and watch cartoons that are in English. You can also practice speaking English with friends or using language learning apps. Reading books and practicing writing in English can help too. Remember, it's important to practice every day and not be afraid to make mistakes. Keep trying and you will get better and better at English!\n"
     ]
    }
   ],
   "source": [
    "response = completion1['choices'][0]['message']['content']\n",
    "print(response)\n",
    "\n",
    "# Oh wow! Learning English is super duper fun! Here are some tips to help you learn:\n",
    "\n",
    "# 1. Read English books or articles out loud to improve your pronunciation and fluency.\n",
    "\n",
    "# 2. Watch English films or TV shows with subtitles to help you understand and pick up new vocabulary.\n",
    "\n",
    "# 3. Practice speaking with native English speakers, either in person or online, to improve your grammar and speaking skills.\n",
    "\n",
    "# 4. Listen to English music and try to sing along to improve your listening and speaking skills.\n",
    "\n",
    "# 5. Make flashcards with English words and their meanings and review them often to improve your vocabulary.\n",
    "\n",
    "# 6. Find a language exchange partner to practice speaking with on a regular basis.\n",
    "\n",
    "# Remember, practice makes perfect! Good luck with your English learning adventure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iygiWql-2Bh3"
   },
   "outputs": [],
   "source": [
    "query = \"괌으로 여행가려고 하는데, 준비할 물품좀 알려줘\"\n",
    "\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0oztNbCI2Fq8",
    "outputId": "8c7f9faf-16c3-481f-e1aa-41fc2c4f9254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국과 같은 문화권이므로 기본적인 준비물품은 크게 다르지 않겠지만, 다음과 같은 물품들을 추천드립니다.\n",
      "\n",
      "1. 여권\n",
      "2. 항공권\n",
      "3. 신용카드 또는 현금\n",
      "4. 전기 어댑터 (미국식 110V 전기 플러그로 변환용 어댑터)\n",
      "5. 선크림 및 모자 등의 일회용품\n",
      "6. 비행기 내부에서 사용할 수 있는 책, 음악 플레이어, 스마트폰, 태블릿 등 기기\n",
      "7. 인삼, 홍삼 등의 건강보조 식품\n",
      "8. 다목적 의약품 (해열제, 소화제 등)\n",
      "9. 수영복, 비치웨어 등의 해변용 의류\n",
      "10. 카메라나 셀카봉 등의 기록용품\n",
      "\n",
      "괌의 날씨는 평균 온도가 27도 정도이므로, 겉옷이나 단도착용에 유의해주세요. 또한, 여행을 갈 때 반드시 1회용 마스크를 챙겨 놓는 것이 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "response = completion['choices'][0]['message']['content']\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggFEgibc3Kbj"
   },
   "outputs": [],
   "source": [
    "# 사용: ref: https://platform.openai.com/account/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQazTbjNG14c"
   },
   "source": [
    "## alphaca-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26dR4Hz_G2Rp"
   },
   "outputs": [],
   "source": [
    "# https://github.com/tatsu-lab/stanford_alpaca\n",
    "!git clone https://github.com/tloen/alpaca-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJcvIkNpG615"
   },
   "outputs": [],
   "source": [
    "# !pip install -r alpaca-lora/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lev8E2SZ8wOn"
   },
   "outputs": [],
   "source": [
    "!mv alpaca-lora/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AuC3ZO_G64f"
   },
   "outputs": [],
   "source": [
    "!python generate.py \\\n",
    "    --load_8bit \\\n",
    "    --base_model 'decapoda-research/llama-7b-hf' \\\n",
    "    --lora_weights 'tloen/alpaca-lora-7b' \\\n",
    "    --share_gradio True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98ZruIm8G69v"
   },
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/1eWAmesrW99p7e1nah5bipn0zikMb8XYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480,
     "referenced_widgets": [
      "096822deaae649799354174880bcd357",
      "8b4c927ffe354151a6dcd2bb7935bb29",
      "072104db651c4b92be835293934a28e5",
      "8d7b632d1d4e4b16aa5bc7b015ac23bf",
      "29c2c62e6fdc4ecfab17e6c94a717078",
      "e7ef55bed16f493b9a4119acc0ca5513",
      "8ab1cc1e25f3415391a6e2f9174db6e8",
      "4dc1fbe8d06340668b889922fe981c42",
      "e34bd82ec25947d581f5a8362af95620",
      "2c718f8431a542f5a8a3f02803758a95",
      "89c80905a33a480fac325880875b443b"
     ]
    },
    "id": "4b2ARfF8G7AV",
    "outputId": "12f05212-09ef-48ec-a68d-49f15790a68f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-1otmqekxijwxp --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096822deaae649799354174880bcd357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/huggingface/diffusers/issues/1207\n",
    "# conda install cudatoolkit\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import pdb\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "assert (\n",
    "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
    "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"    \n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"decapoda-research/llama-7b-hf\",\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map= \"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, \"tloen/alpaca-lora-7b\", torch_dtype=torch.float16\n",
    ").to(device)\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     model, \"tloen/alpaca-lora-7b\", torch_dtype=torch.float16, device_map={'': 0}\n",
    "# )    \n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "trans_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\").to(device)\n",
    "trans_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXDY-WntAsmf"
   },
   "outputs": [],
   "source": [
    "def translate(text, src, tgt):\n",
    "    trans_tokenizer.src_lang = src\n",
    "    encoded = trans_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    generated_tokens = trans_model.generate(**encoded, forced_bos_token_id=trans_tokenizer.get_lang_id(tgt))\n",
    "    result = trans_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def evaluate(\n",
    "    instruction,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=2, # 4\n",
    "    input=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    prompt = generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=2048,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output.split(\"### Response:\")[1].strip()\n",
    "    \n",
    "def generate_fascampus(instruction, ko_query):\n",
    "    print(\"지침:\", instruction)\n",
    "    \n",
    "    print(\"질의:\", ko_query)\n",
    "    en_query = translate(ko_query, 'ko', 'en')\n",
    "    \n",
    "    en_result = evaluate(instruction=instruction, num_beams=2, input=en_query)\n",
    "    print(\"영어응답:\", en_result)\n",
    "    ko_result = translate(en_result, 'en', 'ko')\n",
    "    print(\"한글응답:\", ko_result)\n",
    "    print()\n",
    "    return en_result, ko_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "G3PtpoMmG7C1",
    "outputId": "6e6d9cf6-f532-413f-d939-320d97415b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공부할 질의: 영어\n",
      "지침: 5 ways to study\n",
      "질의: 영어\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어응답: Five ways to study English are: \n",
      "1. Read books and articles related to English. \n",
      "2. Listen to podcasts and audiobooks related to English. \n",
      "3. Watch movies and TV shows in English. \n",
      "4. Practice speaking English with a native speaker. \n",
      "5. Take an English class or online course.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한글응답: 영어를 공부하는 다섯 가지 방법은 다음과 같습니다 : 1. 영어와 관련된 책과 기사를 읽으십시오. 2. 영어와 관련된 팟캐스트와 오디오 책을 듣으십시오. 3. 영어로 영화와 TV 쇼를 보십시오. 4. 원주민 연설자와 함께 영어를 연습하십시오. 5. 영어 수업이나 온라인 코스를 취하십시오.\n",
      "\n",
      "공부할 질의: 수학 연구\n",
      "지침: 5 ways to study\n",
      "질의: 수학 연구\n",
      "영어응답: Five ways to study for Mathematical Studies are:\n",
      "1. Read the textbook and take notes.\n",
      "2. Make flashcards to help memorize key concepts.\n",
      "3. Practice problem-solving exercises.\n",
      "4. Take practice tests.\n",
      "5. Seek help from a teacher or tutor.\n",
      "한글응답: 수학 연구를 위해 공부하는 다섯 가지 방법은 다음과 같습니다 : 1. 책을 읽고 노트를 가져 가십시오. 2. 핵심 개념을 기억하는 데 도움이되는 플래시 카드를 만드십시오. 3. 문제 해결 연습을 실천하십시오. 4. 실습 테스트를 수행하십시오. 5. 교사 또는 교사의 도움을 구하십시오.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 5&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.9/dist-packages/ipykernel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">851</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">raw_input</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">848 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> StdinNotImplementedError(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">849 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"raw_input was called, but this frontend does not support input requests</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">850 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>851 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._input_request(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(prompt),                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">852 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._parent_ident,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">853 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._parent_header,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">854 │   │   │   </span>password=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.9/dist-packages/ipykernel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">kernelbase.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">895</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_input_request</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">892 │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">893 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">894 │   │   │   │   # re-raise KeyboardInterrupt, to truncate traceback</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>895 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">KeyboardInterrupt</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Interrupted by user\"</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">None</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">896 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">897 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.log.warning(<span style=\"color: #808000; text-decoration-color: #808000\">\"Invalid Message:\"</span>, exc_info=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">898 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt: </span>Interrupted by user\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 5>\u001b[0m:\u001b[94m5\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/ipykernel/\u001b[0m\u001b[1;33mkernelbase.py\u001b[0m:\u001b[94m851\u001b[0m in \u001b[92mraw_input\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m848 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m StdinNotImplementedError(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m849 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mraw_input was called, but this frontend does not support input requests\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m850 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m851 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._input_request(\u001b[96mstr\u001b[0m(prompt),                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m852 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._parent_ident,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m853 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._parent_header,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m854 \u001b[0m\u001b[2m│   │   │   \u001b[0mpassword=\u001b[94mFalse\u001b[0m,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/ipykernel/\u001b[0m\u001b[1;33mkernelbase.py\u001b[0m:\u001b[94m895\u001b[0m in \u001b[92m_input_request\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m892 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m893 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m:                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m894 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m895 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mKeyboardInterrupt\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInterrupted by user\u001b[0m\u001b[33m\"\u001b[0m) \u001b[94mfrom\u001b[0m \u001b[96mNone\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m896 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m897 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.log.warning(\u001b[33m\"\u001b[0m\u001b[33mInvalid Message:\u001b[0m\u001b[33m\"\u001b[0m, exc_info=\u001b[94mTrue\u001b[0m)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m898 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt: \u001b[0mInterrupted by user\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "find_num = 5\n",
    "instruction = f\"{find_num} ways to study\"\n",
    "\n",
    "while True:\n",
    "    ko_query = input(\"공부할 질의: \")\n",
    "    if ko_query == 'exit':\n",
    "        break\n",
    "    en_result, ko_result = generate_fascampus(instruction, ko_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4G0ZEtkKZEgH"
   },
   "source": [
    "## opt 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77xTHJM-Y-xZ"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NtE9-p_H4HE_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", torch_dtype=torch.float16).cuda() # opt-30b # 175b\n",
    "\n",
    "# the fast tokenizer currently does not work correctly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTgCAcy84LuR"
   },
   "outputs": [],
   "source": [
    "prompt = \"I want to learn Enlgish.\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# set_seed(32)\n",
    "generated_ids = model.generate(input_ids, do_sample=True)\n",
    "\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0uJqV_B4N5R"
   },
   "outputs": [],
   "source": [
    "## llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7wfiAnQ1jeU"
   },
   "source": [
    "## llama 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkWBSU69Y-YQ"
   },
   "outputs": [],
   "source": [
    "# https://github.com/shawwn/llama-dl?fbclid=IwAR21gvRDtH38tCgS1iW-gU-_CEVLkB7N-rIg-LfVaDvCTkQ1e6c450SZty8\n",
    "!curl -o- https://raw.githubusercontent.com/shawwn/llama-dl/56f50b96072f42fb2520b1ad5a1d6ef30351f23c/llama.sh | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ufJLg_Z9v1"
   },
   "source": [
    "## PEFT 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rfdhcibxEA9"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FoB19eJaDej"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2FBkbLnaS-r"
   },
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVKFqJFOaTXz"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "# output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFjmdzipcSvS"
   },
   "outputs": [],
   "source": [
    "## 출력할 모델 파타리터 함수\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynB7doxocIMH"
   },
   "outputs": [],
   "source": [
    "## 한국어 모델도 당연히 비슷하게 지원\n",
    "## https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5\n",
    "from transformers import AutoModel\n",
    "model_name_or_path = \"skt/ko-gpt-trinity-1.2B-v0.5\" # \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"skt/ko-gpt-trinity-1.2B-v0.5\" # \"bigscience/mt0-large\"\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLdfYEOxbbTu"
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"FastCampus\", inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uZ7-TCDbcEs"
   },
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jCKxsOaagoX"
   },
   "outputs": [],
   "source": [
    "# 학습과정은 기존하고 똑같이 진행\n",
    "model.save_pretrained(\"output_dir\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lNc6XQya9T3"
   },
   "outputs": [],
   "source": [
    "## 공식으로 제공하는 colab (학습 과정, 로딩과정) 참고\n",
    "https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing#scrollTo=otj46qRbtpnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruXLzNi0agrB"
   },
   "outputs": [],
   "source": [
    "## LoRA로 학습된 모델 로딩 예시\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = \"smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83gqXC32nhBU"
   },
   "outputs": [],
   "source": [
    "## peft 활용한 모델은 loading에 아래부분 추가되는 듯함\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "device = torch.device('cpu') # cuda\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "inputs = tokenizer(\"Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :\", return_tensors=\"pt\")\n",
    "\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=10)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y35xlr9hfP0N"
   },
   "outputs": [],
   "source": [
    "## 양자화: https://huggingface.co/docs/transformers/main/en/main_classes/quantization#finetune-a-model-that-has-been-loaded-in-8bit\n",
    "## 좀 더 자세히 알아보고 싶다면? https://github.com/huggingface/peft"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "072104db651c4b92be835293934a28e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4dc1fbe8d06340668b889922fe981c42",
      "max": 33,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e34bd82ec25947d581f5a8362af95620",
      "value": 33
     }
    },
    "096822deaae649799354174880bcd357": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8b4c927ffe354151a6dcd2bb7935bb29",
       "IPY_MODEL_072104db651c4b92be835293934a28e5",
       "IPY_MODEL_8d7b632d1d4e4b16aa5bc7b015ac23bf"
      ],
      "layout": "IPY_MODEL_29c2c62e6fdc4ecfab17e6c94a717078"
     }
    },
    "29c2c62e6fdc4ecfab17e6c94a717078": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c718f8431a542f5a8a3f02803758a95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4dc1fbe8d06340668b889922fe981c42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89c80905a33a480fac325880875b443b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ab1cc1e25f3415391a6e2f9174db6e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b4c927ffe354151a6dcd2bb7935bb29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7ef55bed16f493b9a4119acc0ca5513",
      "placeholder": "​",
      "style": "IPY_MODEL_8ab1cc1e25f3415391a6e2f9174db6e8",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "8d7b632d1d4e4b16aa5bc7b015ac23bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c718f8431a542f5a8a3f02803758a95",
      "placeholder": "​",
      "style": "IPY_MODEL_89c80905a33a480fac325880875b443b",
      "value": " 33/33 [01:12&lt;00:00,  2.47s/it]"
     }
    },
    "e34bd82ec25947d581f5a8362af95620": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7ef55bed16f493b9a4119acc0ca5513": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
